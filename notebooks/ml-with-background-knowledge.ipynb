{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b323495",
   "metadata": {
    "cell_marker": "\"\"\"",
    "id": "0b323495",
    "lines_to_next_cell": 0
   },
   "source": [
    "# mOWL: Python library for machine learning with ontologies\n",
    "\n",
    "## Ontology creation:\n",
    "To get started, you can install mOWL using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eb89b3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5eb89b3",
    "lines_to_next_cell": 2,
    "outputId": "6a54b6a2-7d28-440b-e57f-e2c7bf0f5de7"
   },
   "outputs": [],
   "source": [
    "!pip install mowl-borg==0.1.1 pystow==0.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c111bec",
   "metadata": {
    "cell_marker": "\"\"\"",
    "id": "7c111bec"
   },
   "source": [
    "mOWL interfaces the OWL API. For this, we need to interface with the Java Virtual Machine (JVM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5daeef",
   "metadata": {
    "id": "7a5daeef"
   },
   "outputs": [],
   "source": [
    "import mowl\n",
    "mowl.init_jvm(\"10g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d97b252",
   "metadata": {
    "id": "8d97b252",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from java.util import HashSet\n",
    "from mowl.owlapi import OWLAPIAdapter\n",
    "from org.semanticweb.owlapi.model import IRI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea2cd6c",
   "metadata": {
    "id": "cea2cd6c"
   },
   "source": [
    "## Let's create our first ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1007ec47",
   "metadata": {
    "id": "1007ec47"
   },
   "outputs": [],
   "source": [
    "adapter = OWLAPIAdapter()\n",
    "ontology = adapter.create_ontology(\"http://mowl/family\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e059ced",
   "metadata": {
    "id": "8e059ced"
   },
   "source": [
    "## Class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d1e266",
   "metadata": {
    "id": "75d1e266"
   },
   "outputs": [],
   "source": [
    "male = adapter.create_class(\"http://Male\")\n",
    "female = adapter.create_class(\"http://Female\")\n",
    "parent = adapter.create_class(\"http://Parent\")\n",
    "person = adapter.create_class(\"http://Person\")\n",
    "mother = adapter.create_class(\"http://Mother\")\n",
    "father = adapter.create_class(\"http://Father\")\n",
    "sibling = adapter.create_class(\"http://Sibling\")\n",
    "brother = adapter.create_class(\"http://Brother\")\n",
    "sister = adapter.create_class(\"http://Sister\")\n",
    "son = adapter.create_class(\"http://Son\")\n",
    "daughter = adapter.create_class(\"http://Daughter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d29db1f",
   "metadata": {
    "id": "5d29db1f"
   },
   "source": [
    "## Role names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d7dc08",
   "metadata": {
    "id": "d6d7dc08"
   },
   "outputs": [],
   "source": [
    "has_child = adapter.create_object_property(\"http://hasChild\")\n",
    "has_parent = adapter.create_object_property(\"http://hasParent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517b015f",
   "metadata": {
    "id": "517b015f"
   },
   "source": [
    "## Individual names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d9db1",
   "metadata": {
    "id": "0a8d9db1"
   },
   "outputs": [],
   "source": [
    "John = adapter.create_individual(\"http://John\")\n",
    "Jane = adapter.create_individual(\"http://Jane\")\n",
    "Robert = adapter.create_individual(\"http://Robert\")\n",
    "Melissa = adapter.create_individual(\"http://Melissa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c310f5",
   "metadata": {
    "id": "b1c310f5",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Axioms\n",
    "\n",
    "Let's create some axioms of the form $A \\sqsubseteq B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce4a755",
   "metadata": {
    "id": "5ce4a755"
   },
   "outputs": [],
   "source": [
    "axioms = HashSet()\n",
    "axioms.add(adapter.create_subclass_of(male, person))\n",
    "axioms.add(adapter.create_subclass_of(female, person))\n",
    "axioms.add(adapter.create_subclass_of(parent, person))\n",
    "axioms.add(adapter.create_subclass_of(mother, female))\n",
    "axioms.add(adapter.create_subclass_of(father, male))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca35154",
   "metadata": {
    "id": "6ca35154"
   },
   "source": [
    "Now, let's create some axioms of the form $A \\sqcap B \\sqsubseteq C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f2474",
   "metadata": {
    "id": "aa2f2474"
   },
   "outputs": [],
   "source": [
    "parent_and_male = adapter.create_object_intersection_of(parent, male)\n",
    "axioms.add(adapter.create_subclass_of(parent_and_male, father))\n",
    "parent_and_female = adapter.create_object_intersection_of(parent, female)\n",
    "axioms.add(adapter.create_subclass_of(parent_and_female, mother))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fd7448",
   "metadata": {
    "id": "05fd7448"
   },
   "source": [
    "Now some axioms of the form $A \\sqcup B \\equiv C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd459f4b",
   "metadata": {
    "id": "bd459f4b"
   },
   "outputs": [],
   "source": [
    "male_or_female = adapter.create_object_union_of(male, female)\n",
    "axioms.add(adapter.create_equivalent_classes(male_or_female, person))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3629a3a8",
   "metadata": {
    "id": "3629a3a8"
   },
   "source": [
    "One axiom of the form $\\neg A \\equiv  B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9040b2f",
   "metadata": {
    "id": "d9040b2f"
   },
   "outputs": [],
   "source": [
    "not_male = adapter.create_complement_of(male)\n",
    "axioms.add(adapter.create_equivalent_classes(not_male, female))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e24920",
   "metadata": {
    "id": "c1e24920"
   },
   "source": [
    "One axiom of the form $A \\sqsubseteq \\exists R.B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00daf8d",
   "metadata": {
    "id": "d00daf8d"
   },
   "outputs": [],
   "source": [
    "has_child_person = adapter.create_object_some_values_from(has_child, person)\n",
    "axioms.add(adapter.create_subclass_of(parent, has_child_person))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a910184",
   "metadata": {
    "id": "8a910184"
   },
   "source": [
    "And finally, some assertion axioms of the form $C(a)$ and $R(a,b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13955925",
   "metadata": {
    "id": "13955925"
   },
   "outputs": [],
   "source": [
    "axioms.add(adapter.create_class_assertion(father, John))\n",
    "axioms.add(adapter.create_class_assertion(mother, Jane))\n",
    "axioms.add(adapter.create_class_assertion(male, Robert))\n",
    "axioms.add(adapter.create_class_assertion(female, Melissa))\n",
    "axioms.add(adapter.create_object_property_assertion(has_child, John, Robert))\n",
    "axioms.add(adapter.create_object_property_assertion(has_child, Jane, Robert))\n",
    "axioms.add(adapter.create_object_property_assertion(has_child, John, Melissa))\n",
    "axioms.add(adapter.create_object_property_assertion(has_child, Jane, Melissa))\n",
    "adapter.owl_manager.addAxioms(ontology, axioms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d6a3c",
   "metadata": {
    "id": "140d6a3c"
   },
   "outputs": [],
   "source": [
    "ont_file = os.path.abspath(f'family.owl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3397783",
   "metadata": {
    "id": "a3397783"
   },
   "outputs": [],
   "source": [
    "adapter.owl_manager.saveOntology(ontology, IRI.create('file://'+ont_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f5c99",
   "metadata": {
    "cell_marker": "\"\"\"",
    "id": "280f5c99"
   },
   "source": [
    "# Ontology projections into graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8c98c6",
   "metadata": {
    "cell_marker": "r\"\"\",\n\"\"\"",
    "id": "9e8c98c6"
   },
   "source": [
    "Ontologies are formed by a TBox, an ABox and an RBox. A Knowledge\n",
    "Graph can be easily extracted from the ABox and the RBox. However, to\n",
    "encode the graph representation of the TBox, which is composed by\n",
    "(complex) concept descriptions, many approaches have been developed. In mOWL, we provide some\n",
    "methods that perform ontology projection into graphs:\n",
    "\n",
    "- **Taxonomy projection**: the projection of axioms of the form $A\n",
    "\\sqsubseteq B$ as edges $(A, subclassof, B)$.\n",
    "\n",
    "- **Taxonomy + relations**: the projection of axioms of the form $A\n",
    "\\sqsubseteq B$ and $A \\sqsubseteq \\exists R.B$ as edges $(A,\n",
    "subclassof, B)$ and $(A, R, B)$, respectively.\n",
    "\n",
    "- **DL2Vec projection**\n",
    "\n",
    "- **OWL2Vec projection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2760973",
   "metadata": {
    "id": "e2760973",
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "from mowl.projection import TaxonomyProjector, TaxonomyWithRelationsProjector, DL2VecProjector, OWL2VecStarProjector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385b126d",
   "metadata": {
    "id": "385b126d",
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "#from mowl.datasets.builtin import FamilyDataset\n",
    "from mowl.datasets import PathDataset\n",
    "#dataset = FamilyDataset()\n",
    "dataset = PathDataset(\"family.owl\")\n",
    "edges = TaxonomyProjector().project(dataset.ontology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824a6fd5",
   "metadata": {
    "id": "824a6fd5",
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def nx_network(edges):\n",
    "    import networkx as nx\n",
    "    import matplotlib.pyplot as plt\n",
    "    G = nx.DiGraph()\n",
    "    for edge in edges:\n",
    "        src = edge.src.split(\"/\")[-1]\n",
    "        dst = edge.dst.split(\"/\")[-1]\n",
    "        G.add_edge(src, dst)\n",
    "    #nx draw with custom colors\n",
    "    plt.figure(figsize=(5,5))\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, pos, edge_color='black', width=1, linewidths=1,\n",
    "            node_size=500, node_color='cyan', alpha=0.9,\n",
    "            labels={node:node for node in G.nodes()})\n",
    "    #nx.draw(G, with_labels=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89950b9",
   "metadata": {
    "id": "a89950b9",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "nx_network(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e3c13",
   "metadata": {
    "id": "820e3c13",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "dl2vec_proj = DL2VecProjector(bidirectional_taxonomy=True)\n",
    "d2v_edges = dl2vec_proj.project(dataset.ontology, with_individuals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f4390",
   "metadata": {
    "id": "941f4390",
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "nx_network(d2v_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82628c6",
   "metadata": {
    "id": "b82628c6",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "owl2vec_proj = OWL2VecStarProjector(bidirectional_taxonomy=True)\n",
    "o2v_edges = owl2vec_proj.project(dataset.ontology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dda19de",
   "metadata": {
    "id": "3dda19de",
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "nx_network(o2v_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe12eb68",
   "metadata": {
    "id": "fe12eb68"
   },
   "source": [
    "# Random-walk-based embeddings of ontologies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0da2a4c",
   "metadata": {
    "id": "b0da2a4c"
   },
   "source": [
    "After generating the graph, we can embed it in different ways. Two approaches are supported in mOWL:\n",
    "- Embeddings based on random walks\n",
    "- Embeddings based on KGE\n",
    "\n",
    "Let's try the approach with random walks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0970a958",
   "metadata": {
    "id": "0970a958"
   },
   "outputs": [],
   "source": [
    "from mowl.walking.deepwalk.model import DeepWalk\n",
    "walker =  DeepWalk(\n",
    "             10, #num_walks,\n",
    "             4, #walk_length,\n",
    "             0.1, #alpha\n",
    "             outfile = \"walks_dw.txt\", # /optional/path/to/save/walks,\n",
    "             workers = 4)\n",
    "walker.walk(o2v_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd225ef8",
   "metadata": {
    "id": "bd225ef8"
   },
   "source": [
    "## Process the walks using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acac572",
   "metadata": {
    "id": "7acac572",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "walk_corpus_file = walker.outfile\n",
    "sentences = LineSentence(walk_corpus_file)\n",
    "\n",
    "w2v_model = Word2Vec(sentences, vector_size = 15) #vector_size/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c9e8f0",
   "metadata": {
    "id": "c1c9e8f0"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(\"http://Jane\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6eeac9",
   "metadata": {
    "id": "ec6eeac9"
   },
   "source": [
    "## Task 🚧\n",
    "\n",
    "We will add a new axiom in the ontology:  $hasChild(John,Bob)$. We know that Jhon and Jane already have two children. Can we infer that Bob is also realted to Jane?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9570d6b9",
   "metadata": {
    "id": "9570d6b9"
   },
   "outputs": [],
   "source": [
    "Bob = adapter.create_individual(\"http://Bob\")\n",
    "axioms.add(adapter.create_object_property_assertion(has_child, John, Bob))\n",
    "adapter.owl_manager.addAxioms(ontology, axioms)\n",
    "ont_file = os.path.abspath(f'family2.owl')\n",
    "adapter.owl_manager.saveOntology(ontology, IRI.create('file://'+ont_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acd1595",
   "metadata": {
    "id": "3acd1595"
   },
   "outputs": [],
   "source": [
    "dataset = PathDataset(\"family2.owl\")\n",
    "projector = OWL2VecStarProjector(bidirectional_taxonomy=True) # YOUR CODE HERE\n",
    "new_edges = projector.project(dataset.ontology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7d3f3a",
   "metadata": {
    "id": "9c7d3f3a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "walker =  DeepWalk(\n",
    "             10, #number of walks,\n",
    "             5, #walk length,\n",
    "             0.1, #alpha: restart parameter\n",
    "             outfile = \"walks_dw.txt\", # /optional/path/to/save/walks,\n",
    "             workers = 4)\n",
    "\n",
    "walker.walk(new_edges)\n",
    "walk_corpus_file = walker.outfile\n",
    "sentences = LineSentence(walk_corpus_file)\n",
    "w2v_model = Word2Vec(sentences, vector_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9673582",
   "metadata": {
    "id": "c9673582"
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(\"http://Jane\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3305542",
   "metadata": {
    "cell_marker": "\"\"\"",
    "id": "f3305542"
   },
   "source": [
    "# Syntactic embeddings of ontologies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10bba63",
   "metadata": {
    "cell_marker": "r\"\"\",\n\"\"\"",
    "id": "e10bba63"
   },
   "source": [
    "Syntactic embeddings embedding uses the syntax of axioms to generate sentences out of them. mOWL provides methods to generate text sentences from the axioms and/or the annotations in the ontology. The syntax chosen to generate the sentences is [Manchester Syntax](https://www.w3.org/2007/OWL/draft/ED-owl2-manchester-syntax-20081128/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd5e17c",
   "metadata": {
    "id": "0cd5e17c",
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "import mowl\n",
    "mowl.init_jvm(\"10g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907a848d",
   "metadata": {
    "cell_marker": "\"\"\"",
    "id": "907a848d",
    "lines_to_next_cell": 0
   },
   "source": [
    "We import our `Family Ontology` and the method `extract_axiom_corpus`, which extracts the axioms from the ontology and generates sentences in *Manchester Syntax*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6116391c",
   "metadata": {
    "id": "6116391c",
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "from mowl.corpus import extract_axiom_corpus\n",
    "from mowl.datasets import PathDataset\n",
    "dataset = PathDataset(\"family.owl\")\n",
    "corpus = extract_axiom_corpus(dataset.ontology)\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5b7fc9",
   "metadata": {
    "cell_marker": "\"\"\"",
    "id": "ae5b7fc9"
   },
   "source": [
    "Let's see the corpus generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d4e29",
   "metadata": {
    "id": "2e8d4e29",
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "for s in corpus[:10]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303f0d19",
   "metadata": {
    "cell_marker": "\"\"\"",
    "id": "303f0d19",
    "lines_to_next_cell": 2
   },
   "source": [
    "Now it is possible to input this corpus in a model like Word2Vec, which will generate numerical representations for our vocabulary. We will use the `gensim` library to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d336b243",
   "metadata": {
    "id": "d336b243",
    "lines_to_next_cell": 0,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [s.split(\" \") for s in corpus]\n",
    "w2v = Word2Vec(sentences, epochs=200, vector_size = 50, min_count = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8019828",
   "metadata": {
    "cell_marker": "\"\"\"",
    "id": "f8019828"
   },
   "source": [
    "Finally, we can provide a visual representation of the entities. We will use a modified version of TSNE, which is implemented here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e03dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mowl.visualization import TSNE as MTSNE\n",
    "from sklearn.manifold import TSNE as SKTSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TSNE(MTSNE):\n",
    "\n",
    "    def __init__(self, *args, perplexity=5, thickness = 50,  **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.perplexity = perplexity\n",
    "        self.thickness = thickness\n",
    "\n",
    "    def generate_points(self, epochs, workers=1, verbose=0):\n",
    "        \"\"\"This method will call the :meth:`sklearn.manifold.TSNE.fit_transform`\n",
    "        method to generate the points for the plot.\n",
    "\n",
    "        :param epochs: Number of epochs to run the TSNE algorithm\n",
    "        :type epochs: int\n",
    "        :param workers: Number of workers to use for parallel processing. Defaults to 1.\n",
    "        :type workers: int, optional\n",
    "        :param verbose: Verbosity level. Defaults to 0.\n",
    "        \"\"\"\n",
    "        points = np.array(list(self.embeddings.values()))\n",
    "        if np.iscomplexobj(points):\n",
    "            if verbose:\n",
    "                warnings.warn(\"Complex numpy array detected. Only real part will be considered\",\n",
    "                              UserWarning)\n",
    "            points = points.real\n",
    "        self.points = SKTSNE(n_components=2, verbose=verbose, n_iter=epochs, n_jobs=workers, perplexity=self.perplexity)\n",
    "        self.points = self.points.fit_transform(points)\n",
    "        self.plot_data = {}\n",
    "\n",
    "        for name, idx in self.embedding_idx_dict.items():\n",
    "            label = self.labels[name]\n",
    "            x, y = tuple(self.points[idx])\n",
    "\n",
    "            if label not in self.plot_data:\n",
    "                self.plot_data[label] = [], []\n",
    "            self.plot_data[label][0].append(x)\n",
    "            self.plot_data[label][1].append(y)\n",
    "\n",
    "    def show(self, thickness = None):\n",
    "        \"\"\" This method will call the :meth:`matplotlib.pyplot.show` method to show the plot.\n",
    "        \"\"\"\n",
    "        if thickness is None:\n",
    "            thickness = self.thickness\n",
    "            fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "        for label, (xs, ys) in self.plot_data.items():\n",
    "            color = self.class_color_dict[label]\n",
    "            ax.scatter(xs, ys, color=color, label=label, s=thickness)\n",
    "            ax.text(xs[0]+0.5, ys[0]+0.5, label, fontsize=12)\n",
    "\n",
    "            ax.legend()\n",
    "            ax.grid(True)\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dc7c59",
   "metadata": {
    "id": "39dc7c59"
   },
   "outputs": [],
   "source": [
    "#from scripts.tsne import TSNE\n",
    "\n",
    "vectors = w2v.wv\n",
    "vocab_dict = vectors.key_to_index\n",
    "name_to_label = {c: c.split(\"/\")[-1] for c in vocab_dict if str(c).startswith(\"http://\")}\n",
    "name_to_emb = {c: vectors[[c]][0] for c in name_to_label}\n",
    "\n",
    "tsne = TSNE(name_to_emb, name_to_label)\n",
    "tsne.generate_points(500, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36e9316",
   "metadata": {
    "id": "c36e9316"
   },
   "outputs": [],
   "source": [
    "tsne.show(thickness=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aad3652",
   "metadata": {
    "id": "3aad3652"
   },
   "outputs": [],
   "source": [
    "# built-in imports\n",
    "import sys\n",
    "import torch as th\n",
    "import logging\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from mowl.visualization.base import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from mowl.projection.edge import Edge\n",
    "from mowl.datasets.builtin import GDADataset, GDAHumanDataset, GDAMouseDataset\n",
    "from pykeen.models import TransE,ConvE,DistMult,TransR,TransD\n",
    "from mowl.projection.dl2vec.model import DL2VecProjector\n",
    "from mowl.kge import KGEModel\n",
    "from mowl.evaluation.rank_based import EmbeddingsRankBasedEvaluator\n",
    "from mowl.evaluation.base import TranslationalScore, CosineSimilarity\n",
    "from mowl.projection.factory import projector_factory, PARSING_METHODS\n",
    "from mowl.walking import DeepWalk\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from mowl.evaluation.rank_based import EmbeddingsRankBasedEvaluator\n",
    "from mowl.evaluation.base import CosineSimilarity\n",
    "from mowl.projection import TaxonomyWithRelationsProjector\n",
    "from mowl.projection.edge import Edge\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore',category=UserWarning,module='gensim')\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning,module='gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a069723",
   "metadata": {
    "id": "1a069723"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de7bf71",
   "metadata": {
    "id": "4de7bf71"
   },
   "source": [
    "Use the Built-in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641d839",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "8641d839"
   },
   "outputs": [],
   "source": [
    "dataset = GDAMouseDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d946492",
   "metadata": {
    "id": "8d946492"
   },
   "source": [
    "The dataset will be downloaded to a folder name `gda_mouse` with the training, validation and testing ontology dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1e5d92",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "6d1e5d92"
   },
   "outputs": [],
   "source": [
    "! ls gda_mouse/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda650f7",
   "metadata": {
    "id": "dda650f7"
   },
   "source": [
    "# Graph-based embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6815df2",
   "metadata": {
    "id": "b6815df2"
   },
   "source": [
    " ### Example for two methods: DL2vec and Owl2vec* methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e9d46d",
   "metadata": {
    "id": "97e9d46d"
   },
   "source": [
    "<font color='blue'><font size=\"4\">1) DL2vec Prediction Method </font></font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ea29e",
   "metadata": {
    "id": "752ea29e"
   },
   "source": [
    "1. **Projecting the ontology**\n",
    "- Project the ontology using the DL2Vec Projector class, with the specific rules used to project the ontology.\n",
    "- The outcome of the projection algorithm is an edgelist.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d6d37",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "340d6d37"
   },
   "outputs": [],
   "source": [
    "from mowl.projection.dl2vec.model import DL2VecProjector\n",
    "projector = DL2VecProjector(True)\n",
    "train_edges = projector.project(dataset.ontology)\n",
    "test_edges = projector.project(dataset.testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06738cf5",
   "metadata": {
    "id": "06738cf5"
   },
   "source": [
    "2. **Generating random walks**\n",
    "- The random walks are generated using the DeepWalk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ce47dd",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "02ce47dd"
   },
   "outputs": [],
   "source": [
    "walker = DeepWalk(10, # number of walks per node\n",
    "                  10, # walk length\n",
    "                  0.1, # restart probability\n",
    "                  workers=4, outfile = 'walk',seed=40) # number of threads\n",
    "\n",
    "walks = walker.walk(train_edges)\n",
    "walks_file = walker.outfile\n",
    "sentences = LineSentence(walks_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7c8c52",
   "metadata": {
    "id": "0e7c8c52"
   },
   "source": [
    "3. **Training the Word2Vec model**\n",
    "- To train the Word2Vec model, we rely on the Gensim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f92981e",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7f92981e",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, vector_size=100, epochs = 15, window=5, min_count=1, workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7460a6f",
   "metadata": {
    "id": "f7460a6f"
   },
   "source": [
    "4. **Evaluating the embeddings**\n",
    "- We are going to evaluate the plausibility of an association gene-disease with a gene against all possible diseases and check the rank of the true disease association using CosineSimilarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad2bfc5",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "fad2bfc5"
   },
   "outputs": [],
   "source": [
    "genes, diseases = dataset.evaluation_classes\n",
    "projector = TaxonomyWithRelationsProjector(taxonomy=False,\n",
    "                                           relations=[\"http://is_associated_with\"])\n",
    "\n",
    "vectors = model.wv\n",
    "evaluator = EmbeddingsRankBasedEvaluator(\n",
    "    vectors,\n",
    "    test_edges,\n",
    "    CosineSimilarity,\n",
    "    training_set=train_edges,\n",
    "    head_entities = genes.as_str,\n",
    "    tail_entities = diseases.as_str,\n",
    "    device = 'cpu')\n",
    "\n",
    "\n",
    "evaluator.evaluate(show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b43a2d",
   "metadata": {
    "id": "24b43a2d"
   },
   "outputs": [],
   "source": [
    "human_disease=[]\n",
    "mouse_genes=[]\n",
    "for classes in vectors.index_to_key:\n",
    "    if 'OMIM' in classes:\n",
    "        human_disease.append(classes)\n",
    "    if classes[7:].isnumeric():\n",
    "        mouse_genes.append(classes)\n",
    "\n",
    "print(f'Number of the disease is {len(human_disease)}, and number of genes is {len(mouse_genes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10a0736",
   "metadata": {
    "id": "b10a0736"
   },
   "outputs": [],
   "source": [
    "human_disease[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a85b4",
   "metadata": {
    "id": "bf5a85b4"
   },
   "outputs": [],
   "source": [
    "mouse_genes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4872f771",
   "metadata": {
    "id": "4872f771"
   },
   "outputs": [],
   "source": [
    "human_disease_vectors=[]\n",
    "for k in human_disease:\n",
    "    human_disease_vectors.append(vectors[k])\n",
    "\n",
    "mouse_genes_vectors=[]\n",
    "for k in mouse_genes:\n",
    "    mouse_genes_vectors.append(vectors[k])\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(np.array(human_disease_vectors),np.array(mouse_genes_vectors))\n",
    "\n",
    "print(f\"The dimentions of this matrix is {similarity.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744d9536",
   "metadata": {
    "id": "744d9536"
   },
   "source": [
    "## Evaluating the predictions to find the most similar genes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bfbd1d",
   "metadata": {
    "id": "b9bfbd1d",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def find_similar_genes(disease_id, top_k, disease_genes_similarity_matrix, disease_keys, gene_keys):\n",
    "    disease_index = disease_keys.index(disease_id)\n",
    "    prediction_list = np.flip(np.argsort(disease_genes_similarity_matrix[disease_index]))\n",
    "    top_genes = [gene_keys[prediction_list[x]] for x in range(top_k)]\n",
    "\n",
    "    return top_genes\n",
    "\n",
    "#associations from the file MGI_DO.rpt\n",
    "\n",
    "\n",
    "#DOID:0080449\tdevelopmental and epileptic encephalopathy 16\tOMIM:615338\tmouse, laboratory\t10090\tTbc1d24\t224617\tMGI:2443456\n",
    "disease_id = 'http://OMIM_615338'\n",
    "top_k = find_similar_genes(disease_id, 5 ,similarity, human_disease, mouse_genes )\n",
    "print(f'The most similar gene to disease {disease_id.split(\"/\")[2]} are: {top_k}')\n",
    "\n",
    "\n",
    "#DOID:0080436\tdevelopmental and epileptic encephalopathy 4\tOMIM:612164\thuman\t9606\tSTXBP1\t6812\n",
    "#DOID:0060309\tsyndromic X-linked intellectual disability\t\thuman\t9606\tHNRNPH2\t3188\n",
    "#HNRNPH2\t3188\tHnrnph2\tMGI:1201779\tMP:0001186, MP:0005386, MP:0010771\n",
    "#disease_id = 'http://OMIM_612164'\n",
    "#top_k = find_similar_genes(disease_id, 5 ,similarity, human_disease, mouse_genes )\n",
    "#print(f'The most similar gene to disease {disease_id.split(\"/\")[2]} are: {top_k}')\n",
    "\n",
    "\n",
    "#OMIM_181500 : schizophrenia : DOID:5419\tOMIM:181500\tmouse, laboratory\t10090\tMagi2\t50791\tMGI:1354953\n",
    "#disease_id = 'http://OMIM_181500'\n",
    "#top_k = find_similar_genes(disease_id, 5 ,similarity, human_disease, mouse_genes )\n",
    "#print(f'The most similar gene to disease {disease_id.split(\"/\")[2]} are: {top_k}')\n",
    "\n",
    "\n",
    "#OMIM_615643 : neurodegeneration with brain iron accumulation 6\n",
    "#DOID:0110740\tOMIM:615643\tmouse, laboratory\t10090\tCoasy\t71743\tMGI:1918993\n",
    "#disease_id = 'http://OMIM_615643'\n",
    "#top_k = find_similar_genes(disease_id, 5 ,similarity, human_disease, mouse_genes )\n",
    "#print(f'The most similar gene to disease {disease_id.split(\"/\")[2]} are: {top_k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b2d1c3",
   "metadata": {
    "id": "34b2d1c3",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def find_similar_genes(disease_id, top_k, disease_genes_similarity_matrix, disease_keys, gene_keys):\n",
    "    disease_index = disease_keys.index(disease_id)\n",
    "    prediction_list = np.flip(np.argsort(disease_genes_similarity_matrix[disease_index]))\n",
    "    top_genes = [gene_keys[prediction_list[x]] for x in range(top_k)]\n",
    "\n",
    "    return top_genes\n",
    "\n",
    "disease_id = 'http://OMIM_615643'\n",
    "top_k = find_similar_genes(disease_id, 5 ,similarity, human_disease, mouse_genes )\n",
    "print(f'The most similar gene to disease {disease_id.split(\"/\")[2]} are: {top_k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c32f44",
   "metadata": {
    "id": "a7c32f44"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88ee013",
   "metadata": {
    "id": "f88ee013"
   },
   "source": [
    "# **Task 1 :**\n",
    "\n",
    "<div class=\"alert alert-block alert-success\" , color ='grreen'>\n",
    "\n",
    "<font size=\"4\">\n",
    "    Predict the <font color='SteelBlue'>top 10 similar genes</font> to\n",
    "    diabetes mellitus disease OMIM ID: <font color='Tomato'>http://OMIM_608036</font>\n",
    "    using <font color='red'>OWL2vec*</font> prediction method\n",
    "\n",
    "</font>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a188eb",
   "metadata": {
    "id": "70a188eb"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Follow the <b>TODO</b> interactions to modify the script, and the rest should be the same you just need to run the cell to execute the code.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699ca37f",
   "metadata": {
    "id": "699ca37f"
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8b5745",
   "metadata": {
    "id": "2d8b5745"
   },
   "source": [
    "<font color='blue'><font size=\"4\">2) OWL2vec* Prediction Method </font></font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa6828e",
   "metadata": {
    "id": "9fa6828e"
   },
   "source": [
    "1. **Projecting the ontology**\n",
    "- Project the ontology using the OWL2Vec* Projector class, with the specific rules used to project the ontology.\n",
    "- The outcome of the projection algorithm is an edgelist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296ce30d",
   "metadata": {
    "id": "296ce30d"
   },
   "outputs": [],
   "source": [
    "from mowl.projection import #TODO: import the appropriate function (refer to https://mowl.readthedocs.io/en/latest/api/projection/index.html)\n",
    "dataset = GDAMouseDataset()\n",
    "projector = OWL2VecStarProjector(True)\n",
    "train_edges = projector.project(dataset.ontology)\n",
    "test_edges = projector.project(dataset.testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b87aff2",
   "metadata": {
    "id": "1b87aff2"
   },
   "source": [
    "2. **Generating random walks**\n",
    "- The random walks are generated using the DeepWalk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045e0f4e",
   "metadata": {
    "id": "045e0f4e"
   },
   "outputs": [],
   "source": [
    "walker = DeepWalk( ,#TODO: add the number of walks per node\n",
    "                   ,#TODO: add the walk length\n",
    "                  workers=4, # number of threads\n",
    "                  outfile = , #TODO: add the name of the output file for the walks\n",
    "                  seed=40) #fix the random seed\n",
    "\n",
    "walks = walker.walk(train_edges)\n",
    "walks_file = walker.outfile\n",
    "sentences = LineSentence(walks_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d2a7b0",
   "metadata": {
    "id": "81d2a7b0"
   },
   "source": [
    "3. **Training the Word2Vec model**\n",
    "- To train the Word2Vec model, we rely on the Gensim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484bd016",
   "metadata": {
    "id": "484bd016",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences,\n",
    "                 vector_size= , #TODO: add the size of the vector\n",
    "                 epochs = ,     #TODO: update the number of training epochs\n",
    "                 window=5, min_count=1, workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60da1b5",
   "metadata": {
    "id": "d60da1b5"
   },
   "source": [
    "4. **Evaluating the embeddings**\n",
    "- We are going to evaluate the plausibility of an association gene-disease with a gene against all possible diseases and check the rank of the true disease association using CosineSimilarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c46c3d",
   "metadata": {
    "id": "74c46c3d"
   },
   "outputs": [],
   "source": [
    "genes, diseases = dataset.evaluation_classes\n",
    "projector = TaxonomyWithRelationsProjector(taxonomy=False,\n",
    "                                           relations=[\"http://is_associated_with\"])\n",
    "\n",
    "vectors = model.wv\n",
    "evaluator = EmbeddingsRankBasedEvaluator(\n",
    "    vectors,\n",
    "    test_edges,\n",
    "    CosineSimilarity,\n",
    "    training_set=train_edges,\n",
    "    head_entities = genes.as_str,\n",
    "    tail_entities = diseases.as_str,\n",
    "    device = 'cuda')\n",
    "\n",
    "evaluator.evaluate(show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d8d678",
   "metadata": {
    "id": "53d8d678"
   },
   "outputs": [],
   "source": [
    "human_disease=[]\n",
    "mouse_genes=[]\n",
    "for classes in vectors.index_to_key:\n",
    "    if 'OMIM' in classes:\n",
    "        human_disease.append(classes)\n",
    "    if classes[7:].isnumeric():\n",
    "        mouse_genes.append(classes)\n",
    "\n",
    "print(f'Number of the disease is {len(human_disease)}, and number of genes is {len(mouse_genes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab6429",
   "metadata": {
    "id": "bdab6429"
   },
   "outputs": [],
   "source": [
    "human_disease_vectors=[]\n",
    "for k in human_disease:\n",
    "    human_disease_vectors.append(vectors[k])\n",
    "\n",
    "mouse_genes_vectors=[]\n",
    "for k in mouse_genes:\n",
    "    mouse_genes_vectors.append(vectors[k])\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(np.array(human_disease_vectors),np.array(mouse_genes_vectors))\n",
    "\n",
    "print(\"the dimentions of this matrix is \", similarity.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12029bf7",
   "metadata": {
    "id": "12029bf7"
   },
   "source": [
    "## Evaluating the predictions to find the most similar genes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766634df",
   "metadata": {
    "id": "766634df",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def find_similar_genes(disease_id, top_k, disease_genes_similarity_matrix, disease_keys, gene_keys):\n",
    "    disease_index = disease_keys.index(disease_id)\n",
    "    prediction_list = np.flip(np.argsort(disease_genes_similarity_matrix[disease_index]))\n",
    "    top_genes = [gene_keys[prediction_list[x]] for x in range(top_k)]\n",
    "\n",
    "    return top_genes\n",
    "\n",
    "\n",
    "\n",
    "disease_id = #TODO: write the disease OMIM ID\n",
    "\n",
    "number_of_genes =  #TODO: number of genes to be ranked\n",
    "\n",
    "top_k = find_similar_genes( , #TODO: disease OMIM ID\n",
    "                            , #TODO: number of genes\n",
    "                           similarity,\n",
    "                           human_disease,\n",
    "                           mouse_genes)\n",
    "\n",
    "print(f'The top {number_of_genes} most similar gene to disease {disease_id.split(\"/\")[2]} are:')\n",
    "\n",
    "for idx, genes in enumerate(top_k):\n",
    "    print(f\" Gene in Rank {idx+1} is : {top_k[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e6c9cb",
   "metadata": {
    "id": "09e6c9cb"
   },
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287856bb",
   "metadata": {
    "id": "287856bb"
   },
   "source": [
    "# Syntactic embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a861a5e",
   "metadata": {
    "id": "3a861a5e"
   },
   "outputs": [],
   "source": [
    "from mowl.corpus import extract_and_save_axiom_corpus\n",
    "from mowl.owlapi import OWLAPIAdapter\n",
    "from mowl.reasoning import MOWLReasoner\n",
    "from org.semanticweb.elk.owlapi import ElkReasonerFactory\n",
    "from java.util import HashSet\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import Word2Vec\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3259db3b",
   "metadata": {
    "id": "3259db3b"
   },
   "source": [
    "<font color='blue'><font size=\"4\">1) Onto2Vec Prediction Method </font></font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470de88a",
   "metadata": {
    "id": "470de88a"
   },
   "source": [
    "This example corresponds to the paper **Onto2Vec: joint vector-based representation of biological entities and their ontology-based annotations**.\n",
    "\n",
    "This method is an approach to learn numerical representations (embeddings) of (biomedical) ontologies by representing ontology axioms as text sequences and applying an unsupervised learning algorithm such as Word2Vec. Onto2Vec uses an ontology reasoner to infer new axioms as a preprocessing step. The algorithm is tested on the protein-protein interaction task.\n",
    "\n",
    "For this algorithm, we need three components:\n",
    "\n",
    "1. The reasoner\n",
    "\n",
    "2. The corpus generator\n",
    "\n",
    "3. The Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28c09d3",
   "metadata": {
    "id": "d28c09d3"
   },
   "source": [
    "\n",
    "**1) Inferring new axioms**\n",
    "\n",
    "- Onto2Vec uses an ontology reasoner to infer new axioms as a preprocessing step. In the original paper, the authors used the HermiT reasoner. For this example, we use the ELK reasoner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f5f81f",
   "metadata": {
    "id": "07f5f81f",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "reasoner_factory = ElkReasonerFactory()\n",
    "reasoner = reasoner_factory.createReasoner(dataset.ontology)\n",
    "mowl_reasoner = MOWLReasoner(reasoner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a607124b",
   "metadata": {
    "id": "a607124b",
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa479c3d",
   "metadata": {
    "id": "aa479c3d"
   },
   "outputs": [],
   "source": [
    "# We wrap the reasoner into the :class:`MOWLReasoner <mowl.reasoning.base.MOWLReasoner>` class \\\n",
    "# in order to use some shortcuts the mOWL\n",
    "# provides such as:\n",
    "#\n",
    "# - inferring subclass axioms\n",
    "# - inferring equivalent class axioms\n",
    "# - inferring disjoint axioms (not applicable for this example since we use ELK reasoner)\n",
    "\n",
    "classes = dataset.ontology.getClassesInSignature()\n",
    "subclass_axioms = mowl_reasoner.infer_subclass_axioms(classes)\n",
    "equivalent_class_axioms = mowl_reasoner.infer_equivalent_class_axioms(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccda2cf",
   "metadata": {
    "id": "4ccda2cf"
   },
   "outputs": [],
   "source": [
    "# We can now add the inferred axioms to the ontology.\n",
    "\n",
    "adapter = OWLAPIAdapter()\n",
    "manager = adapter.owl_manager\n",
    "\n",
    "axioms = HashSet()\n",
    "axioms.addAll(subclass_axioms)\n",
    "axioms.addAll(equivalent_class_axioms)\n",
    "\n",
    "manager.addAxioms(dataset.ontology, axioms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6100653",
   "metadata": {
    "id": "d6100653"
   },
   "source": [
    "**2- The corpus generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f38b4f",
   "metadata": {
    "id": "e6f38b4f"
   },
   "outputs": [],
   "source": [
    "extract_and_save_axiom_corpus(dataset.ontology, \"onto2vec_corpus.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e1569",
   "metadata": {
    "id": "007e1569"
   },
   "source": [
    "**3- Generating the corpus and training the model**\n",
    "- Now that we have an extended ontology, we can generate the corpus out of it. After that, we can train the Word2Vec model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd1ba4d",
   "metadata": {
    "id": "bcd1ba4d"
   },
   "outputs": [],
   "source": [
    "sentences = LineSentence(\"onto2vec_corpus.txt\")\n",
    "model_onto = Word2Vec(sentences, vector_size=5, epochs=10, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f600d556",
   "metadata": {
    "id": "f600d556"
   },
   "outputs": [],
   "source": [
    "# Cleaning up memory\n",
    "# os.remove(\"onto2vec_corpus.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5e8e3a",
   "metadata": {
    "id": "6f5e8e3a"
   },
   "source": [
    "## Evaluating the embeddings\n",
    "- We are going to evaluate the plausibility of an association gene-disease with a gene against all possible diseases and check the rank of the true disease association using CosineSimilarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c822bc",
   "metadata": {
    "id": "28c822bc"
   },
   "outputs": [],
   "source": [
    "genes, diseases = dataset.evaluation_classes\n",
    "projector = TaxonomyWithRelationsProjector(taxonomy=False,\n",
    "                                           relations=[\"http://is_associated_with\"])\n",
    "\n",
    "vectors = model_onto.wv\n",
    "evaluator = EmbeddingsRankBasedEvaluator(\n",
    "    vectors,\n",
    "    test_edges,\n",
    "    CosineSimilarity,\n",
    "    training_set=train_edges,\n",
    "    head_entities = genes.as_str,\n",
    "    tail_entities = diseases.as_str,\n",
    "    device = 'cpu')\n",
    "\n",
    "evaluator.evaluate(show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7686321",
   "metadata": {
    "id": "a7686321",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "human_disease=[]\n",
    "mouse_genes=[]\n",
    "for classes in vectors.index_to_key:\n",
    "    if 'OMIM' in classes:\n",
    "        human_disease.append(classes)\n",
    "    if classes[7:].isnumeric():\n",
    "        mouse_genes.append(classes)\n",
    "\n",
    "human_disease_vectors=[]\n",
    "for k in human_disease:\n",
    "    human_disease_vectors.append(vectors[k])\n",
    "\n",
    "mouse_genes_vectors=[]\n",
    "for k in mouse_genes:\n",
    "    mouse_genes_vectors.append(vectors[k])\n",
    "\n",
    "similarity = cosine_similarity(np.array(human_disease_vectors),np.array(mouse_genes_vectors))\n",
    "\n",
    "disease_id = 'http://OMIM_612164'\n",
    "top_k = find_similar_genes(disease_id, 10 ,similarity, human_disease, mouse_genes )\n",
    "print(f'The most similar gene to disease {disease_id.split(\"/\")[2]} are: {top_k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f92cef",
   "metadata": {
    "id": "b5f92cef"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87223b25",
   "metadata": {
    "id": "87223b25"
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72266288",
   "metadata": {
    "id": "72266288"
   },
   "source": [
    "# **Task 2 :**\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "<font size=\"4\">\n",
    "    Predict the <font color='SteelBlue'>top 5 similar genes</font> to\n",
    "    diabetes mellitus disease OMIM ID: <font color='Tomato'>http://OMIM_608036</font>\n",
    "    using <font color='red'>OPA2Vec*</font> prediction method\n",
    "\n",
    "</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6433d7",
   "metadata": {
    "id": "6d6433d7"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Follow the <b>TODO</b> interactions to modify the script, and the rest should be the same you just need to run the cell to execute the code.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be3754b",
   "metadata": {
    "id": "8be3754b"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a701653",
   "metadata": {
    "id": "0a701653"
   },
   "source": [
    "<font color='blue'><font size=\"4\">2) OPA2Vec Prediction Method </font></font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ce206f",
   "metadata": {
    "id": "d7ce206f"
   },
   "source": [
    "This example corresponds to the paper **OPA2Vec: combining formal and informal content of biomedical ontologies to improve similarity-based prediction**.\n",
    "\n",
    "This method is an extension of **Onto2Vec** that apart from formal knowldege (i.e. axioms) it also uses informal knowledge such as entity metadata (i.e. synonyms, definitions, etc.)\n",
    "\n",
    "For this algorithm, we need four components:\n",
    "\n",
    "1.  The reasoner\n",
    "\n",
    "2. The corpus generator\n",
    "\n",
    "<font color='red'>3. The annotations generator</font>\n",
    "\n",
    "4. The Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafe0009",
   "metadata": {
    "id": "aafe0009"
   },
   "outputs": [],
   "source": [
    "from mowl.corpus import extract_and_save_axiom_corpus\n",
    "from mowl.owlapi import OWLAPIAdapter\n",
    "from mowl.reasoning import MOWLReasoner\n",
    "from org.semanticweb.elk.owlapi import ElkReasonerFactory\n",
    "from java.util import HashSet\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from mowl.corpus import extract_and_save_axiom_corpus, extract_and_save_annotation_corpus\n",
    "# OPA2Vec use annotation so we need to import extract_and_save_annotation_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6772b8a4",
   "metadata": {
    "id": "6772b8a4"
   },
   "source": [
    "\n",
    "**1) Inferring new axioms**\n",
    "\n",
    "- OPA2Vec uses an ontology reasoner to infer new axioms as a preprocessing step. In the original paper, the authors used the HermiT reasoner. For this example, we use the ELK reasoner.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f545f4c7",
   "metadata": {
    "id": "f545f4c7"
   },
   "outputs": [],
   "source": [
    "reasoner_factory = ElkReasonerFactory()\n",
    "reasoner = reasoner_factory.createReasoner(dataset.ontology)\n",
    "mowl_reasoner = MOWLReasoner(reasoner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66962b3b",
   "metadata": {
    "id": "66962b3b"
   },
   "source": [
    "We wrap the reasoner into the **MOWLReasoner** class in order to use some shortcuts the mOWL provides such as:\n",
    "\n",
    "- inferring subclass axioms\n",
    "\n",
    "- inferring equivalent class axioms\n",
    "\n",
    "- inferring disjoint axioms (not applicable for this example since we use ELK reasoner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bda387f",
   "metadata": {
    "id": "6bda387f"
   },
   "outputs": [],
   "source": [
    "classes = dataset.ontology.getClassesInSignature()\n",
    "subclass_axioms = mowl_reasoner.infer_subclass_axioms(classes)\n",
    "equivalent_class_axioms = mowl_reasoner.infer_equivalent_class_axioms(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd40b64",
   "metadata": {
    "id": "3fd40b64"
   },
   "source": [
    "We can now add the inferred axioms to the ontology:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6675739d",
   "metadata": {
    "id": "6675739d"
   },
   "source": [
    "**2- The corpus generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c0791c",
   "metadata": {
    "id": "c6c0791c"
   },
   "outputs": [],
   "source": [
    "adapter = OWLAPIAdapter()\n",
    "manager = adapter.owl_manager\n",
    "\n",
    "axioms = HashSet()\n",
    "axioms.addAll(subclass_axioms)\n",
    "axioms.addAll(equivalent_class_axioms)\n",
    "\n",
    "manager.addAxioms(dataset.ontology, axioms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e5596c",
   "metadata": {
    "id": "29e5596c"
   },
   "outputs": [],
   "source": [
    "extract_and_save_axiom_corpus(dataset.ontology, \"opa2vec_corpus.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe48e6",
   "metadata": {
    "id": "6dbe48e6"
   },
   "source": [
    "<font color='red'> **3- The annotations generator**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e564c2fc",
   "metadata": {
    "id": "e564c2fc",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#TODO : extract and save the annotations (dataset.ontology, \"opa2vec_corpus.txt\", mode=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e7a9d",
   "metadata": {
    "id": "ca3e7a9d"
   },
   "source": [
    "**4- Generating the corpus and training the model**\n",
    "- Now that we have an extended ontology, we can generate the corpus out of it. After that, we can train the Word2Vec model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d91ba4",
   "metadata": {
    "id": "22d91ba4",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "sentences = LineSentence(\"opa2vec_corpus.txt\")\n",
    "\n",
    "model = Word2Vec(sentences,\n",
    "                 vector_size= , #TODO: add the size of the vector\n",
    "                 epochs = ,     #TODO: update the number of training epochs\n",
    "                 window=5, min_count=1, workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39542626",
   "metadata": {
    "id": "39542626"
   },
   "source": [
    "## Evaluating the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa966427",
   "metadata": {
    "id": "aa966427"
   },
   "outputs": [],
   "source": [
    "genes, diseases = dataset.evaluation_classes\n",
    "projector = TaxonomyWithRelationsProjector(taxonomy=False,\n",
    "                                           relations=[\"http://is_associated_with\"])\n",
    "\n",
    "eval_train_edges = projector.project(dataset.ontology)\n",
    "eval_test_edges = projector.project(dataset.testing)\n",
    "\n",
    "vectors = model.wv\n",
    "evaluator = EmbeddingsRankBasedEvaluator(\n",
    "    vectors,\n",
    "    eval_test_edges,\n",
    "    CosineSimilarity,\n",
    "    training_set=eval_train_edges,\n",
    "    head_entities = genes.as_str,\n",
    "    tail_entities = diseases.as_str,\n",
    "    device = 'cpu')\n",
    "\n",
    "evaluator.evaluate(show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6468a8c2",
   "metadata": {
    "id": "6468a8c2",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def find_similar_genes(disease_id, top_k, disease_genes_similarity_matrix, disease_keys, gene_keys):\n",
    "    disease_index = disease_keys.index(disease_id)\n",
    "    prediction_list = np.flip(np.argsort(disease_genes_similarity_matrix[disease_index]))\n",
    "    top_genes = [gene_keys[prediction_list[x]] for x in range(top_k)]\n",
    "\n",
    "    return top_genes\n",
    "\n",
    "\n",
    "vectors = model.wv\n",
    "\n",
    "\n",
    "human_disease=[]\n",
    "mouse_genes=[]\n",
    "for classes in vectors.index_to_key:\n",
    "    if 'OMIM' in classes:\n",
    "        human_disease.append(classes)\n",
    "    if 'http://' in classes and classes[7:].isnumeric():\n",
    "        mouse_genes.append(classes)\n",
    "\n",
    "human_disease_vectors=[]\n",
    "for k in human_disease:\n",
    "    human_disease_vectors.append(vectors[k])\n",
    "\n",
    "mouse_genes_vectors=[]\n",
    "for k in mouse_genes:\n",
    "    mouse_genes_vectors.append(vectors[k])\n",
    "\n",
    "similarity = cosine_similarity(np.array(human_disease_vectors),np.array(mouse_genes_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a463ed6",
   "metadata": {
    "id": "1a463ed6",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "disease_id =  #TODO: write the disease OMIM ID\n",
    "\n",
    "number_of_genes = #TODO: number of genes to be ranked\n",
    "\n",
    "top_k = find_similar_genes( , #TODO: disease OMIM ID\n",
    "                            , #TODO: number of genes\n",
    "                           similarity,\n",
    "                           human_disease,\n",
    "                           mouse_genes)\n",
    "\n",
    "print(f'The top {number_of_genes} most similar gene to disease {disease_id.split(\"/\")[2]} are:')\n",
    "\n",
    "for idx, genes in enumerate(top_k):\n",
    "    print(f\" Gene in Rank {idx+1} is : {top_k[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bbb752",
   "metadata": {
    "id": "13bbb752"
   },
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f288b89b",
   "metadata": {
    "id": "f288b89b"
   },
   "source": [
    "# Ontologies and Text-mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05225846",
   "metadata": {
    "id": "05225846"
   },
   "source": [
    "Genes and Diseases extracted from text using the exact match, refer to the notebook [Ontologies_and_text_mining](https://github.com/bio-ontology-research-group/mowl-tutorial/blob/main/notebooks/02_Ontologies_and_text_mining.ipynb), to see how the model trained, and for more details refer to the presentation [Ontologies and Text-mining](https://github.com/bio-ontology-research-group/mowl-tutorial/blob/main/slides/Ontologies%20and%20text%20mining.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35610ead",
   "metadata": {
    "id": "35610ead",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pickle as pkl\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0082f0",
   "metadata": {
    "id": "5b0082f0"
   },
   "outputs": [],
   "source": [
    "# Collect the vectors for the genes and diseases\n",
    "word2vec_file = 'w2v_model/wv_model'\n",
    "vectors = Word2Vec.load(word2vec_file)\n",
    "\n",
    "human_disease=[]\n",
    "mouse_genes=[]\n",
    "for classes in vectors.wv.index_to_key:\n",
    "    if 'OMIM_' in classes:\n",
    "        human_disease.append(classes)\n",
    "    if 'http://' in classes and classes[7:].isnumeric():\n",
    "        mouse_genes.append(classes)\n",
    "\n",
    "human_disease_vectors=[]\n",
    "for k in human_disease:\n",
    "    human_disease_vectors.append(vectors.wv[k])\n",
    "\n",
    "mouse_genes_vectors=[]\n",
    "for k in mouse_genes:\n",
    "    mouse_genes_vectors.append(vectors.wv[k])\n",
    "\n",
    "similarity = cosine_similarity(np.array(human_disease_vectors),np.array(mouse_genes_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d343200c",
   "metadata": {
    "id": "d343200c"
   },
   "source": [
    "## Evaluating the predictions to find the most similar genes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466460e8",
   "metadata": {
    "id": "466460e8"
   },
   "outputs": [],
   "source": [
    "# Evaluating the similarity\n",
    "\n",
    "def find_similar_genes(disease_id, top_k, disease_genes_similarity_matrix, disease_keys, gene_keys):\n",
    "    disease_index = disease_keys.index(disease_id)\n",
    "    prediction_list = np.flip(np.argsort(disease_genes_similarity_matrix[disease_index]))\n",
    "    top_genes = [gene_keys[prediction_list[x]] for x in range(top_k)]\n",
    "\n",
    "    return top_genes\n",
    "\n",
    "disease_id = 'http://OMIM_114500'\n",
    "# DOID:9256\tcolorectal cancer\tOMIM:114500\thuman\t9606\tKDR\t3791\n",
    "\n",
    "\n",
    "top_k = find_similar_genes(disease_id, 10 ,similarity, human_disease, mouse_genes )\n",
    "print(f'The most similar genes to disease {disease_id.split(\"/\")[2]} are:')\n",
    "for i, j in enumerate(top_k):\n",
    "    print(i+1, j)\n",
    "\n",
    "# The linked genes at rank 7\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f8d951",
   "metadata": {
    "id": "89f8d951"
   },
   "source": [
    "# Model-theoretic ontology embedding methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2af1d37",
   "metadata": {
    "id": "a2af1d37"
   },
   "source": [
    "## EL-Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc6e535",
   "metadata": {
    "id": "1dc6e535"
   },
   "source": [
    "Import MOWL library and ELEmbedding model base classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df3c656",
   "metadata": {
    "id": "4df3c656",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from mowl.models.elembeddings.module import ELEmModule\n",
    "from mowl.base_models.elmodel import EmbeddingELModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f7e1c7",
   "metadata": {
    "id": "47f7e1c7"
   },
   "source": [
    "Define the model and training strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1b7cb6",
   "metadata": {
    "id": "ff1b7cb6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "\n",
    "class ELEmbeddings(EmbeddingELModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 embed_dim=50,\n",
    "                 margin=0,\n",
    "                 reg_norm=1,\n",
    "                 learning_rate=0.001,\n",
    "                 epochs=1000,\n",
    "                 batch_size=4096 * 8,\n",
    "                 model_filepath=None,\n",
    "                 device='cpu'\n",
    "                 ):\n",
    "        super().__init__(dataset, batch_size, extended=True, model_filepath=model_filepath)\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.margin = margin\n",
    "        self.reg_norm = reg_norm\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        self._loaded = False\n",
    "        self._loaded_eval = False\n",
    "        self.extended = False\n",
    "        self.init_model()\n",
    "\n",
    "    def init_model(self):\n",
    "        self.model = ELEmModule(\n",
    "            len(self.class_index_dict),  # number of ontology classes\n",
    "            len(self.object_property_index_dict),  # number of ontology object properties\n",
    "            embed_dim=self.embed_dim,\n",
    "            margin=self.margin\n",
    "        ).to(self.device)\n",
    "\n",
    "    def train(self, checkpoint=1):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        for epoch in trange(self.epochs):\n",
    "            self.model.train()\n",
    "\n",
    "            train_loss = 0\n",
    "            loss = 0\n",
    "\n",
    "            # Notice how we use the ``training_datasets`` variable directly\n",
    "            # and every element of it is a pair (GCI name, GCI tensor data).\n",
    "            for gci_name, gci_dataset in self.training_datasets.items():\n",
    "                if len(gci_dataset) == 0:\n",
    "                    continue\n",
    "                loss += torch.mean(self.model(gci_dataset[:], gci_name))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.detach().item()\n",
    "            torch.save(self.model.state_dict(), self.model_filepath)\n",
    "            if (epoch + 1) % checkpoint == 0:\n",
    "                print(f'\\nEpoch {epoch}: Train loss: {train_loss:4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb16de69",
   "metadata": {
    "id": "fb16de69"
   },
   "source": [
    "Create the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a6410",
   "metadata": {
    "id": "8e7a6410"
   },
   "outputs": [],
   "source": [
    "from mowl.datasets import PathDataset\n",
    "\n",
    "family_dataset = PathDataset('family.owl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d14c0",
   "metadata": {
    "id": "b31d14c0"
   },
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c9f4d",
   "metadata": {
    "id": "301c9f4d"
   },
   "outputs": [],
   "source": [
    "elembeddings = ELEmbeddings(family_dataset,\n",
    "                     embed_dim=2,\n",
    "                     margin=0.1,\n",
    "                     reg_norm=1,\n",
    "                     learning_rate=0.01,\n",
    "                     epochs=1000,\n",
    "                     batch_size=2,\n",
    "                     model_filepath=None,\n",
    "                     device='cpu')\n",
    "\n",
    "elembeddings.train(checkpoint=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef579cf3",
   "metadata": {
    "id": "ef579cf3"
   },
   "source": [
    "Extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a8e650",
   "metadata": {
    "id": "f4a8e650"
   },
   "outputs": [],
   "source": [
    "embeds = elembeddings.model.class_embed.weight.cpu().detach().numpy()\n",
    "rs = np.abs(elembeddings.model.class_rad.weight.cpu().detach().numpy())\n",
    "classes = list(elembeddings.class_index_dict.keys())\n",
    "rs, embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf199ac",
   "metadata": {
    "id": "9bf199ac"
   },
   "source": [
    "Plot embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e759f",
   "metadata": {
    "id": "6e8e759f",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classes = [item.split('/')[-1] for item in classes]\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "fig, ax =  plt.subplots()\n",
    "plt.axis('equal')\n",
    "ax.set_xlim(-5, 4)\n",
    "ax.set_ylim(-3, 4)\n",
    "for i in range(embeds.shape[0]):\n",
    "    if classes[i].endswith('hing'):\n",
    "        continue\n",
    "    x, y = embeds[i, 0], embeds[i, 1]\n",
    "    r = rs[i]\n",
    "    ax.add_artist(plt.Circle(\n",
    "        (x, y), r, fill=False, edgecolor=colors[i % len(colors)], label=classes[i]))\n",
    "    ax.annotate(classes[i], xy=(x, y + r + 0.03), fontsize=10, ha=\"center\", color=colors[i % len(colors)])\n",
    "ax.grid(True)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd4d72a",
   "metadata": {
    "id": "7dd4d72a",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
